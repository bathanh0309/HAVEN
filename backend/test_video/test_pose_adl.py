"""
HAVEN - Pose + ADL Combined Test (FULLY OPTIMIZED + GIF Export)
================================================================
C·∫£i ti·∫øn ho√†n ch·ªânh:
‚úÖ Majority voting cho t∆∞ th·∫ø ·ªïn ƒë·ªãnh
‚úÖ Ph√°t hi·ªán s·ª± ki·ªán: Fall, Hand Raise, Sitting Down, Standing Up
‚úÖ Logic ph√¢n lo·∫°i t∆∞ th·∫ø c·∫£i thi·ªán v·ªõi g√≥c ƒë·∫ßu g·ªëi
‚úÖ M√†u s·∫Øc skeleton ch√≠nh x√°c (head/torso/upper_arm/lower_arm/upper_leg/lower_leg)
‚úÖ Chu·∫©n h√≥a ng∆∞·ª°ng theo k√≠ch th∆∞·ªõc frame
‚úÖ Xu·∫•t file GIF v·ªõi ph√≠m G
"""

import cv2
import time
import math
import numpy as np
from pathlib import Path
from collections import deque, Counter
from typing import Dict, List, Tuple, Optional
from PIL import Image
from ultralytics import YOLO

# Import all configurations
from config import *

# All configurations imported from config.py

# ============================================================
# L·ªöP QU·∫¢N L√ù TR·∫†NG TH√ÅI THEO D√ïI (TRACKING)
# ============================================================
class TrackState:
    """L∆∞u tr·ªØ l·ªãch s·ª≠ v√† tr·∫°ng th√°i c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng ƒëang ƒë∆∞·ª£c theo d√µi"""
    
    def __init__(self, track_id: int, frame_height: int):
        self.track_id = track_id
        self.frame_height = frame_height
        
        # L·ªãch s·ª≠ v·ªã tr√≠ v√† t∆∞ th·∫ø
        self.positions: deque = deque(maxlen=POSITION_HISTORY_MAXLEN)
        self.postures: deque = deque(maxlen=POSTURE_HISTORY_MAXLEN)  # ‚úÖ TH·ª∞C S·ª∞ S·ª¨ D·ª§NG
        
        # Tr·∫°ng th√°i hi·ªán t·∫°i
        self.current_posture = "UNKNOWN"
        self.prev_posture = "UNKNOWN"
        self.posture_start_time = time.time()
        
        # Theo d√µi chuy·ªÉn ƒë·ªông
        self.last_center = None
        self.velocity = 0.0
        
        # S·ª± ki·ªán
        self.events: deque = deque(maxlen=EVENT_HISTORY_MAXLEN)
        
        # Theo d√µi tay gi∆°
        self.left_hand_raised_frames = 0
        self.right_hand_raised_frames = 0
        
    def update_position(self, bbox: List[float]):
        """C·∫≠p nh·∫≠t v·ªã tr√≠ m·ªõi v√† t√≠nh to√°n t·ªëc ƒë·ªô t·ª©c th·ªùi"""
        cx = (bbox[0] + bbox[2]) / 2
        cy = (bbox[1] + bbox[3]) / 2
        current_time = time.time()
        
        if self.last_center is not None:
            dx = cx - self.last_center[0]
            dy = cy - self.last_center[1]
            dist = math.sqrt(dx*dx + dy*dy)
            self.velocity = dist * ASSUMED_FPS  # pixels/second
        
        self.last_center = (cx, cy)
        self.positions.append((current_time, cx, cy))
        
    def get_movement_in_window(self, window_sec: float = 1.0) -> float:
        """T√≠nh t·ªïng qu√£ng ƒë∆∞·ªùng di chuy·ªÉn trong m·ªôt kho·∫£ng th·ªùi gian"""
        if len(self.positions) < 2:
            return 0.0
            
        now = time.time()
        total_dist = 0.0
        prev = None
        
        for t, x, y in self.positions:
            if now - t > window_sec:
                continue
            if prev is not None:
                dx = x - prev[0]
                dy = y - prev[1]
                total_dist += math.sqrt(dx*dx + dy*dy)
            prev = (x, y)
            
        return total_dist
    
    def add_posture(self, posture: str):
        """‚úÖ TH√äM T∆Ø TH·∫æ V√Ä TH·ª∞C HI·ªÜN MAJORITY VOTING"""
        self.postures.append(posture)
        
        if len(self.postures) >= POSTURE_VOTING_MIN_FRAMES:
            # B·ªè phi·∫øu - l·∫•y t∆∞ th·∫ø xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
            counter = Counter(self.postures)
            voted_posture = counter.most_common(1)[0][0]
            
            # Ph√°t hi·ªán chuy·ªÉn ƒë·ªïi t∆∞ th·∫ø
            if voted_posture != self.current_posture:
                self.prev_posture = self.current_posture
                self.current_posture = voted_posture
                self.posture_start_time = time.time()
                
                # ‚úÖ PH√ÅT HI·ªÜN S·ª∞ KI·ªÜN CHUY·ªÇN ƒê·ªîI
                if self.prev_posture == "STANDING" and voted_posture == "SITTING":
                    self.add_event(EVENT_NAMES["sitting_down"])
                elif self.prev_posture == "SITTING" and voted_posture == "STANDING":
                    self.add_event(EVENT_NAMES["standing_up"])
                elif voted_posture == "LAYING" and self.prev_posture not in ["LAYING", "UNKNOWN"]:
                    self.add_event(EVENT_NAMES["fall_down"])
        else:
            # Ch∆∞a ƒë·ªß d·ªØ li·ªáu voting
            self.current_posture = posture
    
    def add_event(self, event: str):
        """Th√™m s·ª± ki·ªán m·ªõi"""
        timestamp = time.strftime("%H:%M:%S")
        event_text = f"{timestamp} - {event}"
        self.events.append(event_text)
        print(f"[ID:{self.track_id}] EVENT: {event}")
    
    def check_hand_raise(self, keypoints: np.ndarray) -> Optional[str]:
        """‚úÖ PH√ÅT HI·ªÜN GI∆† TAY"""
        def get_point(idx):
            if keypoints[idx][2] > KEYPOINT_CONF_THRESHOLD:
                return keypoints[idx][:2]
            return None
        
        l_wrist = get_point(KP_LEFT_WRIST)
        r_wrist = get_point(KP_RIGHT_WRIST)
        l_shoulder = get_point(KP_LEFT_SHOULDER)
        r_shoulder = get_point(KP_RIGHT_SHOULDER)
        nose = get_point(KP_NOSE)
        
        if nose is None:
            return None
        
        # Ki·ªÉm tra tay tr√°i
        if l_wrist is not None and l_shoulder is not None:
            if l_wrist[1] < nose[1]:  # Y nh·ªè h∆°n = cao h∆°n
                self.left_hand_raised_frames += 1
                if self.left_hand_raised_frames == HAND_RAISE_FRAMES_THRESHOLD:
                    return EVENT_NAMES["left_hand_raised"]
            else:
                self.left_hand_raised_frames = 0
        
        # Ki·ªÉm tra tay ph·∫£i
        if r_wrist is not None and r_shoulder is not None:
            if r_wrist[1] < nose[1]:
                self.right_hand_raised_frames += 1
                if self.right_hand_raised_frames == HAND_RAISE_FRAMES_THRESHOLD:
                    return EVENT_NAMES["right_hand_raised"]
            else:
                self.right_hand_raised_frames = 0
        
        return None

# ============================================================
# TRACKER D·ª∞A TR√äN IOU
# ============================================================
class SimpleTracker:
    """B·ªô theo d√µi ƒë·ªëi t∆∞·ª£ng d·ª±a tr√™n ƒë·ªô tr√πng kh·ªõp Bounding Box (IOU)"""
    
    def __init__(self, iou_threshold=TRACKER_IOU_THRESHOLD, max_age=TRACKER_MAX_AGE, frame_height=720):
        self.tracks: Dict[int, dict] = {}
        self.states: Dict[int, TrackState] = {}
        self.next_id = 1
        self.max_age = max_age
        self.iou_threshold = iou_threshold
        self.ages: Dict[int, int] = {}
        self.frame_height = frame_height
        
    def update(self, detections: List[dict]) -> List[dict]:
        """G√°n ID cho c√°c ph√°t hi·ªán m·ªõi d·ª±a tr√™n l·ªãch s·ª≠"""
        matched_ids = set()
        results = []
        
        for det in detections:
            bbox = det['bbox']
            best_iou = 0
            best_id = None
            
            # So s√°nh v·ªõi c√°c track hi·ªán c√≥
            for tid, track in self.tracks.items():
                if tid in matched_ids:
                    continue
                iou = self._compute_iou(bbox, track['bbox'])
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou = iou
                    best_id = tid
            
            if best_id is not None:
                det['track_id'] = best_id
                self.tracks[best_id] = {'bbox': bbox}
                self.ages[best_id] = 0
                matched_ids.add(best_id)
            else:
                # T·∫°o ID m·ªõi
                det['track_id'] = self.next_id
                self.tracks[self.next_id] = {'bbox': bbox}
                self.states[self.next_id] = TrackState(self.next_id, self.frame_height)
                self.ages[self.next_id] = 0
                self.next_id += 1
            
            # C·∫≠p nh·∫≠t tr·∫°ng th√°i
            tid = det['track_id']
            if tid in self.states:
                self.states[tid].update_position(bbox)
            
            results.append(det)
        
        # X√≥a c√°c track c≈©
        to_remove = []
        for tid in self.tracks:
            if tid not in matched_ids:
                self.ages[tid] = self.ages.get(tid, 0) + 1
                if self.ages[tid] > self.max_age:
                    to_remove.append(tid)
        
        for tid in to_remove:
            del self.tracks[tid]
            if tid in self.states:
                del self.states[tid]
            if tid in self.ages:
                del self.ages[tid]
        
        return results
    
    def _compute_iou(self, box1, box2):
        """T√≠nh to√°n Intersection over Union"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        inter = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - inter
        
        return inter / union if union > 0 else 0.0

# ============================================================
# PH√ÇN LO·∫†I T∆Ø TH·∫æ (POSTURE CLASSIFICATION) - C√ì G√ìC ƒê·∫¶U G·ªêI
# ============================================================
def classify_posture(keypoints: np.ndarray, bbox: List[float], 
                     track_state: TrackState, frame_height: int) -> str:
    """
    ‚úÖ C√ì G√ìC ƒê·∫¶U G·ªêI - PH√ÇN LO·∫†I CH√çNH X√ÅC H∆†N
    """
    if keypoints is None or len(keypoints) < 17:
        return "UNKNOWN"
    
    def get_point(idx):
        if keypoints[idx][2] > KEYPOINT_CONF_THRESHOLD:
            return keypoints[idx][:2]
        return None
    
    # L·∫•y c√°c ƒëi·ªÉm quan tr·ªçng
    l_sh = get_point(KP_LEFT_SHOULDER)
    r_sh = get_point(KP_RIGHT_SHOULDER)
    l_hip = get_point(KP_LEFT_HIP)
    r_hip = get_point(KP_RIGHT_HIP)
    l_knee = get_point(KP_LEFT_KNEE)
    r_knee = get_point(KP_RIGHT_KNEE)
    l_ankle = get_point(KP_LEFT_ANKLE)
    r_ankle = get_point(KP_RIGHT_ANKLE)
    
    # 1. T√≠nh g√≥c th√¢n ng∆∞·ªùi (Torso Angle)
    torso_angle = 0
    if (l_sh is not None or r_sh is not None) and (l_hip is not None or r_hip is not None):
        mid_sh = (l_sh + r_sh) / 2 if l_sh is not None and r_sh is not None else (l_sh if l_sh is not None else r_sh)
        mid_hip = (l_hip + r_hip) / 2 if l_hip is not None and r_hip is not None else (l_hip if l_hip is not None else r_hip)
        
        dx = abs(mid_sh[0] - mid_hip[0])
        dy = abs(mid_sh[1] - mid_hip[1])
        torso_angle = math.degrees(math.atan2(dx, dy)) if dy > 0 else 0
    
    # 2. ‚úÖ T√çNH G√ìC ƒê·∫¶U G·ªêI (Knee Angle) - QUAN TR·ªåNG!
    knee_angles = []
    
    # G√≥c ƒë·∫ßu g·ªëi tr√°i
    if l_hip is not None and l_knee is not None and l_ankle is not None:
        v1 = l_hip - l_knee
        v2 = l_ankle - l_knee
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)  # Tr√°nh l·ªói arccos
        angle = math.degrees(math.acos(cos_angle))
        knee_angles.append(angle)
    
    # G√≥c ƒë·∫ßu g·ªëi ph·∫£i
    if r_hip is not None and r_knee is not None and r_ankle is not None:
        v1 = r_hip - r_knee
        v2 = r_ankle - r_knee
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        angle = math.degrees(math.acos(cos_angle))
        knee_angles.append(angle)
    
    avg_knee_angle = np.mean(knee_angles) if knee_angles else DEFAULT_KNEE_ANGLE
    
    # 3. T√≠nh aspect ratio v√† movement
    w = bbox[2] - bbox[0]
    h = bbox[3] - bbox[1]
    aspect_ratio = w / h if h > 0 else 0
    movement = track_state.get_movement_in_window(1.0)
    
    # ‚úÖ CHU·∫®N H√ìA NG∆Ø·ª†NG THEO FRAME HEIGHT
    movement_threshold = frame_height * MOVEMENT_THRESHOLD_RATIO
    
    # --- LOGIC PH√ÇN LO·∫†I ---
    # N·∫∞M: G√≥c th√¢n l·ªõn ho·∫∑c bbox ngang
    if torso_angle > TORSO_ANGLE_LAYING_THRESHOLD or aspect_ratio > ASPECT_RATIO_LAYING_THRESHOLD:
        return "LAYING"
    
    # NG·ªíI: G√≥c ƒë·∫ßu g·ªëi nh·ªè (g·∫≠p l·∫°i) + √≠t di chuy·ªÉn
    if avg_knee_angle < KNEE_ANGLE_SITTING_THRESHOLD and movement < movement_threshold:
        return "SITTING"
    
    # ƒêI B·ªò: Di chuy·ªÉn nhi·ªÅu
    if movement > movement_threshold * MOVEMENT_WALKING_MULTIPLIER:
        return "WALKING"
    
    # ƒê·ª®NG: M·∫∑c ƒë·ªãnh
    return "STANDING"

# ============================================================
# V·∫º SKELETON - M√ÄU S·∫ÆC CH√çNH X√ÅC
# ============================================================
def draw_skeleton(frame: np.ndarray, keypoints: np.ndarray, conf_threshold: float = KEYPOINT_CONF_THRESHOLD):
    """V·∫Ω b·ªô khung x∆∞∆°ng v·ªõi m√†u s·∫Øc ch√≠nh x√°c cho t·ª´ng b·ªô ph·∫≠n"""
    if keypoints is None or len(keypoints) < 17:
        return
    
    # V·∫Ω c√°c ƒë∆∞·ªùng n·ªëi x∆∞∆°ng
    for (start_idx, end_idx, part) in SKELETON_CONNECTIONS:
        if keypoints[start_idx][2] > conf_threshold and keypoints[end_idx][2] > conf_threshold:
            start_pt = (int(keypoints[start_idx][0]), int(keypoints[start_idx][1]))
            end_pt = (int(keypoints[end_idx][0]), int(keypoints[end_idx][1]))
            color = SKELETON_COLORS.get(part, (255, 255, 255))
            cv2.line(frame, start_pt, end_pt, color, SKELETON_LINE_THICKNESS)
    
    # V·∫Ω c√°c kh·ªõp ƒëi·ªÉm
    for i, kp in enumerate(keypoints):
        if kp[2] > conf_threshold:
            pt = (int(kp[0]), int(kp[1]))
            
            # G√°n m√†u theo v·ªã tr√≠ keypoint
            if i <= 4:  # ƒê·∫ßu
                color = SKELETON_COLORS["head"]
            elif i in [5, 6, 11, 12]:  # Vai v√† h√¥ng (Th√¢n)
                color = SKELETON_COLORS["torso"]
            elif i in [7, 8]:  # C√πi ch·ªè (Tay tr√™n)
                color = SKELETON_COLORS["upper_arm"]
            elif i in [9, 10]:  # C·ªï tay (B√†n tay)
                color = SKELETON_COLORS["lower_arm"]
            elif i in [13, 14]:  # ƒê·∫ßu g·ªëi (Ch√¢n tr√™n)
                color = SKELETON_COLORS["upper_leg"]
            else:  # M·∫Øt c√° (B√†n ch√¢n)
                color = SKELETON_COLORS["lower_leg"]
            
            cv2.circle(frame, pt, SKELETON_KEYPOINT_RADIUS, color, -1)

# ============================================================
# V·∫º DETECTION V·ªöI EVENTS
# ============================================================
def draw_detection(frame: np.ndarray, bbox: List[float], track_id: int, 
                   posture: str, events: deque):
    """V·∫Ω h·ªôp gi·ªõi h·∫°n, nh√£n ADL v√† s·ª± ki·ªán"""
    x1, y1, x2, y2 = map(int, bbox)
    
    color = POSTURE_COLORS.get(posture, (255, 255, 255))
    
    # V·∫Ω BBox
    cv2.rectangle(frame, (x1, y1), (x2, y2), color, BBOX_THICKNESS)
    
    # Nh√£n ch√≠nh
    label = f"ID:{track_id} {posture}"
    (tw, th), _ = cv2.getTextSize(label, FONT_FACE, FONT_SCALE_LABEL, FONT_THICKNESS)
    cv2.rectangle(frame, (x1, y1 - th - 10), (x1 + tw + 10, y1), color, -1)
    cv2.putText(frame, label, (x1 + 5, y1 - 5), FONT_FACE, FONT_SCALE_LABEL, (0, 0, 0), FONT_THICKNESS)
    
    # ‚úÖ HI·ªÇN TH·ªä S·ª∞ KI·ªÜN G·∫¶N NH·∫§T
    if events:
        latest_event = list(events)[-1]
        event_parts = latest_event.split(" - ", 1)
        if len(event_parts) == 2:
            display_text = event_parts[1]  # Ch·ªâ l·∫•y ph·∫ßn t√™n event
        else:
            display_text = latest_event
        
        cv2.putText(frame, display_text, (x1, y2 + 20), 
                   FONT_FACE, FONT_SCALE_EVENT, (0, 0, 255), FONT_THICKNESS)

# ============================================================
# LU·ªíNG CH√çNH (MAIN LOOP) - C√ì GHI GIF
# ============================================================
def main():
    print("=" * 60)
    print("HAVEN - Pose + ADL Combined Test (FULLY OPTIMIZED)")
    print("=" * 60)
    print(f"Model: {MODEL_PATH}")
    print(f"Video: {VIDEO_PATH}")
    print("=" * 60)
    
    # Kh·ªüi t·∫°o model
    model = YOLO(MODEL_PATH)
    cap = cv2.VideoCapture(VIDEO_PATH)
    if not cap.isOpened():
        print("‚ùå L·ªói: Kh√¥ng th·ªÉ m·ªü video")
        return
    
    # L·∫•y th√¥ng tin video
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    
    # Kh·ªüi t·∫°o tracker
    tracker = SimpleTracker(iou_threshold=TRACKER_IOU_THRESHOLD, frame_height=frame_height)
    
    # C·ª≠a s·ªï hi·ªÉn th·ªã
    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(WINDOW_NAME, WINDOW_WIDTH, WINDOW_HEIGHT)
    
    # Tr·∫°ng th√°i
    paused = False
    loop_mode = True
    frame_count = 0
    start_time = time.time()
    
    # ‚úÖ GHI GIF
    recording_gif = False
    gif_frames = []
    
    print("ƒêI·ªÄU KHI·ªÇN:")
    print(f"  {CONTROLS_TEXT['quit']}")
    print(f"  {CONTROLS_TEXT['pause']}")
    print(f"  {CONTROLS_TEXT['loop']}")
    print(f"  {CONTROLS_TEXT['gif']}")
    print("=" * 60)
    
    while True:
        if not paused:
            ret, frame = cap.read()
            if not ret:
                if loop_mode:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue
                else:
                    break
            
            frame_count += 1
            
            # 1. Ch·∫°y YOLO
            results = model(frame, verbose=False, conf=CONF_THRES)[0]
            
            # 2. Ph√¢n t√≠ch k·∫øt qu·∫£
            detections = []
            keypoints_list = []
            if results.boxes is not None:
                boxes = results.boxes.xyxy.cpu().numpy()
                confs = results.boxes.conf.cpu().numpy()
                kpts_data = results.keypoints.data.cpu().numpy() if results.keypoints is not None else None
                
                for i, box in enumerate(boxes):
                    detections.append({'bbox': box.tolist(), 'conf': float(confs[i])})
                    keypoints_list.append(kpts_data[i] if kpts_data is not None and i < len(kpts_data) else None)
            
            # 3. C·∫≠p nh·∫≠t Tracker
            tracked = tracker.update(detections)
            
            # 4. X·ª≠ l√Ω ADL cho t·ª´ng ng∆∞·ªùi
            for i, det in enumerate(tracked):
                tid = det['track_id']
                bbox = det['bbox']
                kpts = keypoints_list[i] if i < len(keypoints_list) else None
                state = tracker.states.get(tid)
                if state is None:
                    continue
                
                # ‚úÖ PH√ÇN LO·∫†I T∆Ø TH·∫æ V·ªöI MAJORITY VOTING
                posture = classify_posture(kpts, bbox, state, frame_height)
                state.add_posture(posture)
                
                # ‚úÖ PH√ÅT HI·ªÜN S·ª∞ KI·ªÜN GI∆† TAY
                if kpts is not None:
                    hand_event = state.check_hand_raise(kpts)
                    if hand_event:
                        state.add_event(hand_event)
                
                # V·∫Ω Skeleton v√† BBox
                if kpts is not None:
                    draw_skeleton(frame, kpts)
                draw_detection(frame, bbox, tid, state.current_posture, state.events)
            
            # 5. Hi·ªÉn th·ªã th√¥ng tin h·ªá th·ªëng
            elapsed = time.time() - start_time
            current_fps = frame_count / elapsed if elapsed > 0 else 0
            cv2.putText(frame, f"FPS: {current_fps:.1f}", (10, 30), 
                       FONT_FACE, FONT_SCALE_INFO, (0, 255, 0), FONT_THICKNESS)
            cv2.putText(frame, f"Persons: {len(tracked)}", (10, 60), 
                       FONT_FACE, FONT_SCALE_INFO, (0, 255, 0), FONT_THICKNESS)
            
            # 6. ‚úÖ GHI GIF
            if recording_gif:
                # Chuy·ªÉn BGR ‚Üí RGB
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # Resize ƒë·ªÉ gi·∫£m dung l∆∞·ª£ng
                h, w = rgb_frame.shape[:2]
                new_w = int(w * GIF_RESIZE_RATIO)
                new_h = int(h * GIF_RESIZE_RATIO)
                small_frame = cv2.resize(rgb_frame, (new_w, new_h))
                gif_frames.append(Image.fromarray(small_frame))
                
                # Hi·ªÉn th·ªã ƒëang ghi - Ch·∫•m ƒë·ªè nh·∫•p nh√°y
                if (frame_count // 10) % 2 == 0:
                    cv2.circle(frame, (frame_width - 30, 30), 10, (0, 0, 255), -1)
                cv2.putText(frame, "REC GIF", (frame_width - 120, 35), 
                           FONT_FACE, FONT_SCALE_LABEL, (0, 0, 255), FONT_THICKNESS)
            
            cv2.imshow(WINDOW_NAME, frame)
        
        # X·ª≠ l√Ω ph√≠m b·∫•m
        key = cv2.waitKey(1 if not paused else 100) & 0xFF
        
        if key == ord('q') or key == ord('Q'):
            break
        elif key == ord(' '):
            paused = not paused
            print(f"{'‚è∏Ô∏è  ƒê√É T·∫†M D·ª™NG' if paused else '‚ñ∂Ô∏è  ƒê√É TI·∫æP T·ª§C'}")
        elif key == ord('l') or key == ord('L'):
            loop_mode = not loop_mode
            print(f"üîÑ Ch·∫ø ƒë·ªô l·∫∑p: {'B·∫¨T' if loop_mode else 'T·∫ÆT'}")
        elif key == ord('g') or key == ord('G'):
            if not recording_gif:
                print("üé¨ B·∫ÆT ƒê·∫¶U GHI GIF...")
                recording_gif = True
                gif_frames = []
            else:
                print("‚èπÔ∏è  D·ª™NG GHI. ƒêANG L∆ØU FILE GIF...")
                recording_gif = False
                
                if len(gif_frames) > 0:
                    # T·∫°o t√™n file v·ªõi timestamp
                    output_filename = f"adl_output_{int(time.time())}.gif"
                    
                    # L∆∞u GIF
                    gif_frames[0].save(
                        output_filename,
                        save_all=True,
                        append_images=gif_frames[1:],
                        optimize=GIF_OPTIMIZE,
                        duration=GIF_DURATION_MS,
                        loop=GIF_LOOP
                    )
                    print(f"‚úÖ ƒê√É L∆ØU: {output_filename} ({len(gif_frames)} frames)")
                else:
                    print("‚ö†Ô∏è  KH√îNG C√ì FRAME N√ÄO ƒê∆Ø·ª¢C GHI.")
    
    cap.release()
    cv2.destroyAllWindows()
    print("\n" + "=" * 60)
    print("ƒê√É ƒê√ìNG ·ª®NG D·ª§NG")
    print("=" * 60)

if __name__ == "__main__":
    main()
    